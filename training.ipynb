{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\n",
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\\Images\n",
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\\SegmentationMasks\n",
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\\ADNI1_T1_Imgs_CN-MCI-AD_5_18_2024.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"Datasets\" / \"ADNI\" / \"ADNI1\"\n",
    "print(dataset_dir)\n",
    "\n",
    "imgs_dir = dataset_dir / \"Images\"\n",
    "masks_dir = dataset_dir / \"SegmentationMasks\"\n",
    "store_dir = dataset_dir / \"InputImages\"\n",
    "input_dir = store_dir\n",
    "\n",
    "print(imgs_dir)\n",
    "print(masks_dir)\n",
    "\n",
    "csv_files = dataset_dir.glob(\"**/*.csv\")\n",
    "for csv_file in csv_files:\n",
    "    csv_data = csv_file\n",
    "    \n",
    "print(csv_data)\n",
    "df = pd.read_csv(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(acq_date):\n",
    "    parsed_date = datetime.strptime(acq_date, f\"%m/%d/%Y\")\n",
    "    return parsed_date.strftime(f\"%Y-%m-%d\")\n",
    "\n",
    "def read_img(parent_dir, subject, date):\n",
    "    input_img_path = parent_dir / f\"{subject}__{date}.nii\"\n",
    "    if not os.path.exists(input_img_path):\n",
    "        return None\n",
    "    input_img = nib.load(input_img_path).get_fdata()\n",
    "    return input_img\n",
    "    # print(input_img.shape)\n",
    "    # plot_axes(input_img, 130)    \n",
    "\n",
    "def plot_16_coronal_slices(img, slice_number):\n",
    "    slices = []\n",
    "    for slice_no in range(slice_number - 7, slice_number + 9):\n",
    "        slices.append(img[:, slice_no, :])# coronal\n",
    "\n",
    "    # print(f\"coronal shape = {slice2.shape}\")\n",
    "    plt.figure()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4)\n",
    "\n",
    "    plt.xticks([])  # Hide x axis ticks and labels\n",
    "    plt.yticks([])  # Hide y axis ticks and labels\n",
    "\n",
    "    # for ax in axes:\n",
    "        # ax.set_xticks([])\n",
    "        # ax.set_yticks([])\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[i][j].imshow(slices[i * 4 + j].T, cmap='gray', origin='lower')\n",
    "            # axes[i][j].set_title(f\"slice {i * 4 + j + 1}\")\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "def read_image(path):\n",
    "    input_img = nib.load(path).get_fdata()\n",
    "    return input_img\n",
    "\n",
    "def separate_16_coronal_slices(img, slice_number):\n",
    "    slices = []\n",
    "    for slice_no in range(slice_number - 14, slice_number + 16):\n",
    "        slices.append(img[:, slice_no, :]) # coronal\n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "113\n",
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "img_paths = []\n",
    "labels = []\n",
    "additional_data = [] # (sex, age)\n",
    "\n",
    "limit = 300\n",
    "cn_limit = 0\n",
    "mci_limit = 0\n",
    "ad_limit = 0\n",
    "\n",
    "input_shapes = {\n",
    "    (256, 256, 166) : 128,\n",
    "    # (192, 192, 160) : {86},\n",
    "    # (240, 256, 160) : {128}\n",
    "}\n",
    "\n",
    "i = 0\n",
    "for index, row in df.iterrows():\n",
    "    group = row[\"Group\"]\n",
    "    # print(group)\n",
    "    # print(count)\n",
    "    \n",
    "    subject = row[\"Subject\"]\n",
    "    date = convert_date(row[\"Acq Date\"])\n",
    "    input_img = read_img(input_dir, subject, date)\n",
    "    \n",
    "    # if image doesn't exist or doesn't follow shape criteria don't add it\n",
    "    if input_img is None or input_img.shape not in input_shapes.keys():\n",
    "        continue\n",
    "\n",
    "    # if i == 1:\n",
    "    #     plot_axes(input_img, input_shapes[input_img.shape])\n",
    "    #     plot_16_coronal_slices(input_img, input_shapes[input_img.shape])\n",
    "    # i = i + 1\n",
    "    if group == \"MCI\":\n",
    "        continue\n",
    "    \n",
    "    if group == \"CN\":\n",
    "        if cn_limit == limit:\n",
    "            continue\n",
    "        cn_limit = cn_limit + 1\n",
    "    else:\n",
    "        if ad_limit == limit:\n",
    "            continue\n",
    "        ad_limit = ad_limit + 1 \n",
    "\n",
    "    sex = 0 if row[\"Sex\"] == 'M' else 1\n",
    "    age = row[\"Age\"]\n",
    "    additional_data.append((sex, age))\n",
    "    # print(additional_data)\n",
    "\n",
    "    input_img_path = input_dir / f\"{subject}__{date}.nii\"\n",
    "    img_paths.append(input_img_path)\n",
    "    labels.append(row[\"Group\"])\n",
    "\n",
    "\n",
    "num_cn = 300\n",
    "num_mci = 187\n",
    "num_ad = 187\n",
    "\n",
    "upsample_count = num_cn - num_ad\n",
    "already_inserted = []\n",
    "\n",
    "for idx, data in enumerate(img_paths):\n",
    "    group = labels[idx]\n",
    "    if group == 'AD' and upsample_count > 0:\n",
    "        upsample_count = upsample_count - 1\n",
    "        if idx not in already_inserted:\n",
    "            already_inserted.append(idx)\n",
    "            img_paths.append(img_paths[idx])\n",
    "            labels.append(labels[idx])\n",
    "            count = count + 1\n",
    "            additional_data.append(additional_data[idx])\n",
    "\n",
    "print(len(additional_data))\n",
    "print(count)\n",
    "print(len(labels))\n",
    "#print(labels[:10])\n",
    "#print(img_paths[:5])\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    input_img = nib.load(path).get_fdata()\n",
    "    return input_img\n",
    "\n",
    "def min_max_normalize(image, new_min=0, new_max=1):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    normalized_image = (image - min_val) / (max_val - min_val) * (new_max - new_min) + new_min\n",
    "    return normalized_image\n",
    "\n",
    "def separate_coronal_slices_around(img, slice_number, slices_count):\n",
    "    slices = []\n",
    "    for slice_no in range(slice_number - slices_count, slice_number + slices_count):\n",
    "        slice = np.rot90(img[:, slice_no, :], k=2)\n",
    "        slice = min_max_normalize(slice)\n",
    "        slices.append(slice) # coronal\n",
    "    return slices\n",
    "\n",
    "class ADNI1_Dataset(Dataset):\n",
    "    def __init__(self, img_paths, additional_data, labels, label_dict, input_shapes):\n",
    "        self.img_paths = img_paths\n",
    "        self.additional_data = additional_data\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.input_shapes = input_shapes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(self.img_paths[idx])\n",
    "        coronal_slices = separate_coronal_slices_around(img, self.input_shapes[img.shape], 4)\n",
    "        coronal_slices = torch.tensor(coronal_slices, dtype=torch.float)\n",
    "        return coronal_slices, torch.tensor(self.additional_data[idx]), self.label_dict[self.labels[idx]]\n",
    "\n",
    "label_dict = {\n",
    "    \"CN\": 0,\n",
    "    \"AD\": 1,\n",
    "}\n",
    "\n",
    "dataset = ADNI1_Dataset(img_paths, additional_data, labels, label_dict, input_shapes)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "dev_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - dev_size\n",
    "\n",
    "train_data, dev_data, test_data = random_split(dataset, [train_size, dev_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "From models/Resnet module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models.ResNet as RN\n",
    "_ = importlib.reload(RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_dict.keys())\n",
    "save_path = \"weights/model_resnet18_8_slices.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RN.ResNet18(device, num_classes)\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(8, model.conv1.out_channels, kernel_size=model.conv1.kernel_size, stride=model.conv1.stride, padding=model.conv1.padding, bias=model.conv1.bias)\n",
    "model.fc2 = torch.nn.Linear(in_features=model.fc2.in_features, out_features=2, bias=True)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"found saved state at: {save_path}\")\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_dict.keys())\n",
    "save_path = \"weights/model_resnet_8_slices.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RN.ResNet18(device, num_classes)\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(8, model.conv1.out_channels, kernel_size=model.conv1.kernel_size, stride=model.conv1.stride, padding=model.conv1.padding, bias=model.conv1.bias)\n",
    "model.fc2 = torch.nn.Linear(in_features=model.fc2.in_features, out_features=2, bias=True)\n",
    "\n",
    "# if os.path.exists(save_path):\n",
    "#     print(f\"found saved state at: {save_path}\")\n",
    "#     checkpoint = torch.load(save_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_best_eval_acc = 0.8733974695205688\n",
    "version = 1\n",
    "iter = 14\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch = {epoch + 1}/{num_epochs}\")\n",
    "    train_running_loss = 0\n",
    "    n = 0\n",
    "    # don't put (data, labels) here instead of data. it botches the self.labels in the loader for some reason\n",
    "    acc = 0\n",
    "    conf_matrix = np.zeros((2, 2))\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        n = n + 1\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"batch id = {batch_idx}\")    \n",
    "            pass\n",
    "        imgs, additional_data, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        outputs = model(imgs, additional_data)\n",
    "\n",
    "        optimizer.zero_grad() # cleans gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # computes gradients\n",
    "        optimizer.step() # applies gradient modifications\n",
    "\n",
    "        train_running_loss += loss.item()\n",
    "        acc = acc + torch.sum(torch.argmax(outputs, dim=1) == labels) / len(labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        conf_matrix += sklearn.metrics.confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=np.arange(2))\n",
    "    \n",
    "    print(conf_matrix)\n",
    "    train_losses.append(train_running_loss / n)\n",
    "    print(f\"train loss = {train_running_loss / n}, acc = {acc / n}\")\n",
    "\n",
    "\n",
    "    dev_running_loss = 0\n",
    "    conf_matrix = np.zeros((2, 2))\n",
    "    n = 0\n",
    "    acc = 0\n",
    "    for i, data in enumerate(dev_loader):\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f\"batch_id = {i}\")\n",
    "        n = n + 1\n",
    "        imgs, additional_data, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        \n",
    "        outputs = model(imgs, additional_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        dev_running_loss += loss.item()    \n",
    "\n",
    "        acc = acc + torch.sum(torch.argmax(outputs, dim=1) == labels) / len(labels)\n",
    "        conf_matrix += sklearn.metrics.confusion_matrix(labels.cpu().numpy(), torch.argmax(outputs, dim=1).cpu().numpy(), labels=np.arange(2))\n",
    "        \n",
    "    print(conf_matrix)\n",
    "    print(f\"dev loss = {dev_running_loss / n}, accuracy = {acc / n}\")\n",
    "    dev_losses.append(dev_running_loss / n)\n",
    "    if acc / n > prev_best_eval_acc:\n",
    "        print(f\"new best acc: {acc / n}\")\n",
    "        version = version + 1\n",
    "        prev_best_eval_acc = acc / n\n",
    "        dt_save_path = f\"weights/model_resnet_8_slices_{iter}_{version}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss\n",
    "        }, dt_save_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prev_best_eval_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_path = 'model_resnet18_8slices.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'loss': loss\n",
    "}, pt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found saved state at: model_resnet18_8slices.pth\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(label_dict.keys())\n",
    "# save_path = \"model_resnet_17.pth\"\n",
    "# save_path = \"model_resnet_best.pth\"\n",
    "# save_path = \"weights/model_resnet_8_slices_14_3.pth\"\n",
    "save_path = \"model_resnet18_8slices.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RN.ResNet18(device, num_classes)\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(8, model.conv1.out_channels, kernel_size=model.conv1.kernel_size, stride=model.conv1.stride, padding=model.conv1.padding, bias=model.conv1.bias)\n",
    "model.fc2 = torch.nn.Linear(in_features=model.fc2.in_features, out_features=2, bias=True)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"found saved state at: {save_path}\")\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\\InputImages\\002_S_0816__2006-08-30.nii\n",
      "(0, 71)\n",
      "AD\n"
     ]
    }
   ],
   "source": [
    "idx = 19\n",
    "print(test_data.dataset.img_paths[idx])\n",
    "print(test_data.dataset.additional_data[idx])\n",
    "print(test_data.dataset.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data, test_data = random_split(dataset, [train_size, dev_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_running_loss = 0\n",
    "conf_matrix = np.zeros((2, 2))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "n = 0\n",
    "acc = 0\n",
    "sens = 0\n",
    "spec = 0\n",
    "auc = 0\n",
    "for i, data in enumerate(test_loader):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"batch_id = {i}\")\n",
    "    n = n + 1\n",
    "    imgs, additional_data, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "    \n",
    "    outputs = model(imgs, additional_data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    test_running_loss += loss.item()    \n",
    "    print(outputs.shape)\n",
    "    print(f\"outputs = {torch.argmax(outputs,dim=1)}\")\n",
    "    print(f\"labels  = {labels}\")\n",
    "    print()\n",
    "    conf_matrix += sklearn.metrics.confusion_matrix(labels.cpu().numpy(), torch.argmax(outputs, dim=1).cpu().numpy(), labels=np.arange(2))\n",
    "\n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    FN = conf_matrix[1, 0]\n",
    "    TP = conf_matrix[1, 1]\n",
    "\n",
    "    sens = sens + TP / (TP + FN)  # True Positive Rate (TPR)\n",
    "    spec = spec + TN / (TN + FP)  # True Negative Rate (TNR)\n",
    "    auc = auc + sklearn.metrics.roc_auc_score(labels.cpu().numpy(), torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "    acc = acc + torch.sum(torch.argmax(outputs, dim=1) == labels) / len(labels)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(f\"eval loss = {test_running_loss / n}, accuracy = {acc / n}\")\n",
    "print(f\"sensitivity = {sens / n}, specificity = {spec / n}, AUC = {auc / n}\")\n",
    "test_losses.append(test_running_loss / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "with open(\"text_3.txt\", 'w') as file:\n",
    "    for loss in train_losses[:150]:\n",
    "        file.write(str(loss) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_losses = []\n",
    "with open(\"text_2.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        _train_losses.append(float(line.strip()))\n",
    "plt.title('Modified ResNet18 - 8 slices')\n",
    "plt.plot(_train_losses[:150])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_losses = []\n",
    "with open(\"text.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        _train_losses.append(float(line.strip()))\n",
    "plt.title('Modified ResNet18 - 1 slice')\n",
    "plt.plot(_train_losses[:150])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Facultate\\anul_3\\ELL\\Code\\Datasets\\ADNI\\ADNI1\\InputImages\\002_S_0685__2007-08-02.nii\n",
      "(1, 91)\n",
      "CN\n",
      "\n",
      "torch.Size([1, 2])\n",
      "outupt = tensor([0], device='cuda:0')\n",
      "\n",
      "label = CN\n",
      "prediction = Cognitively Normal\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "idx = 15\n",
    "print(test_data.dataset.img_paths[idx])\n",
    "print(test_data.dataset.additional_data[idx])\n",
    "print(test_data.dataset.labels[idx])\n",
    "print()\n",
    "\n",
    "predict_labels = {\n",
    "    0: \"Cognitively Normal\",\n",
    "    1: \"Alzheimer's Disease\",\n",
    "}\n",
    "\n",
    "def separate_coronal_slices_around(img, slice_number, slices_count):\n",
    "    slices = []\n",
    "    for slice_no in range(slice_number - slices_count, slice_number + slices_count):\n",
    "        slice = np.rot90(img[:, slice_no, :], k=2)\n",
    "        slice = min_max_normalize(slice)\n",
    "        slices.append(slice) # coronal\n",
    "    return slices\n",
    "\n",
    "def predict(file, additional_data):\n",
    "    img = read_image(file)\n",
    "    if img.shape not in input_shapes:\n",
    "        return \"Incorrect file dimensions, try another\"\n",
    "    coronal_slices = separate_coronal_slices_around(img, input_shapes[img.shape], 4)\n",
    "    coronal_slices = torch.tensor(coronal_slices, dtype=torch.float)\n",
    "\n",
    "    x = coronal_slices.unsqueeze(0).to(device)\n",
    "    y = torch.tensor(additional_data).unsqueeze(0).to(device)\n",
    "    # print(y)\n",
    "    # print(x)\n",
    "    # print(x.shape, y.shape)\n",
    "    model.eval()\n",
    "    output = model(x, y)\n",
    "    print(output.shape)\n",
    "    print(f\"outupt = {torch.argmax(output,dim=1)}\")\n",
    "    print()\n",
    "    # print(torch.argmax(output,dim=1))\n",
    "    return predict_labels[torch.argmax(output,dim=1).item()]\n",
    "\n",
    "pred = predict(test_data.dataset.img_paths[idx], test_data.dataset.additional_data[idx])\n",
    "print(f\"label = {test_data.dataset.labels[idx]}\")\n",
    "print(f\"prediction = {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
